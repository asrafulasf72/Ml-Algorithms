{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiu9sowpSp/VjrBB0TAMMx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asrafulasf72/Ml-Algorithms/blob/main/kNN_calsification_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58Q5RqWwVbXx"
      },
      "outputs": [],
      "source": [
        "# CSE 412 – Lab Activity 2: KNN Classification using scikit-learn\n",
        "# ---------------------------------------------------------------\n",
        "# This script:\n",
        "# 1) Loads Iris data\n",
        "# 2) Splits train/test, scales features, trains KNN\n",
        "# 3) Tries k = 1..15 and records accuracy\n",
        "# 4) Prints confusion matrix & classification report for best k\n",
        "# 5) LAB EX-1: Uses 10-fold CV to get average accuracy for each k, picks the best\n",
        "# 6) LAB EX-2: Tests different train/test ratios and reports which works best (on Iris)\n",
        "# 7) LAB EX-3: Repeats ratio experiment on a small self-created dataset\n",
        "# 8) Predicts two hand-made samples (as in manual)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris, make_classification\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Load dataset & basic info\n",
        "# -----------------------------\n",
        "iris = load_iris()\n",
        "X = iris.data          # shape (150, 4)\n",
        "y = iris.target        # 0=setosa, 1=versicolor, 2=virginica\n",
        "target_names = iris.target_names\n",
        "\n",
        "print(\"Feature names:\", iris.feature_names)\n",
        "print(\"Target names:\", target_names)\n",
        "print(\"Data shape:\", X.shape)   # (150, 4)\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 2) Train/Test split (70/30) + scaling + try k=1..15\n",
        "# ---------------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=0, stratify=y\n",
        ")\n",
        "\n",
        "range_k = range(1, 16)\n",
        "test_accuracies = {}\n",
        "\n",
        "for k in range_k:\n",
        "    pipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=k))\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    test_accuracies[k] = acc\n",
        "\n",
        "# Pick best k on this 70/30 split (if tie, choose smallest k)\n",
        "best_k_split = min(\n",
        "    [k for k, a in test_accuracies.items() if a == max(test_accuracies.values())]\n",
        ")\n",
        "\n",
        "print(\"\\n=== 70/30 split results ===\")\n",
        "for k in range_k:\n",
        "    print(f\"k={k:2d}  | Test Accuracy = {test_accuracies[k]:.4f}\")\n",
        "print(f\"Best k on this split = {best_k_split} (Accuracy={test_accuracies[best_k_split]:.4f})\")\n",
        "\n",
        "# Train with best k and show confusion matrix & classification report\n",
        "best_pipe_split = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=best_k_split))\n",
        "best_pipe_split.fit(X_train, y_train)\n",
        "y_pred_best = best_pipe_split.predict(X_test)\n",
        "\n",
        "print(\"\\nConfusion Matrix (best k on 70/30 split):\")\n",
        "print(confusion_matrix(y_test, y_pred_best))\n",
        "\n",
        "print(\"\\nClassification Report (best k on 70/30 split):\")\n",
        "print(classification_report(y_test, y_pred_best, target_names=target_names))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3) LAB EX-1: 10-fold CV accuracy for each k (average of 10)\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\n=== LAB EX-1: 10-fold CV average accuracy per k ===\")\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "cv_means = {}\n",
        "\n",
        "for k in range_k:\n",
        "    pipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=k))\n",
        "    scores = cross_val_score(pipe, X, y, cv=cv, scoring='accuracy')\n",
        "    cv_means[k] = scores.mean()\n",
        "    print(f\"k={k:2d}  | CV mean accuracy over 10 folds = {cv_means[k]:.4f}\")\n",
        "\n",
        "best_k_cv = min([k for k, a in cv_means.items() if a == max(cv_means.values())])\n",
        "print(f\"Best k by 10-fold CV = {best_k_cv} (Avg Accuracy={cv_means[best_k_cv]:.4f})\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4) LAB EX-2: Best train/test ratio on Iris (repeat 10 times)\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n=== LAB EX-2: Find best train/test ratio on Iris ===\")\n",
        "# We'll evaluate test_size from 0.2 to 0.5\n",
        "ratios = [0.20, 0.30, 0.40, 0.50]\n",
        "ratio_results = {}\n",
        "\n",
        "# Use the best k from CV for a fairer estimate\n",
        "for r in ratios:\n",
        "    accuracies = []\n",
        "    for seed in range(10):  # 10 repeats\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "            X, y, test_size=r, random_state=seed, stratify=y\n",
        "        )\n",
        "        pipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=best_k_cv))\n",
        "        pipe.fit(X_tr, y_tr)\n",
        "        y_hat = pipe.predict(X_te)\n",
        "        accuracies.append(accuracy_score(y_te, y_hat))\n",
        "    ratio_results[r] = (np.mean(accuracies), np.std(accuracies))\n",
        "\n",
        "for r in ratios:\n",
        "    mean_acc, std_acc = ratio_results[r]\n",
        "    print(f\"Test size={int(r*100)}% | Mean Acc over 10 runs = {mean_acc:.4f} ± {std_acc:.4f}\")\n",
        "\n",
        "best_ratio = max(ratio_results, key=lambda rr: ratio_results[rr][0])\n",
        "print(f\"Best test ratio on Iris (by mean acc) = {int(best_ratio*100)}% \"\n",
        "      f\"(Mean Acc={ratio_results[best_ratio][0]:.4f})\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 5) LAB EX-3: Create a small synthetic dataset & repeat ratio test\n",
        "# --------------------------------------------------------------------\n",
        "print(\"\\n=== LAB EX-3: Self-created dataset & best ratio ===\")\n",
        "Xs, ys = make_classification(\n",
        "    n_samples=300, n_features=4, n_informative=3, n_redundant=0,\n",
        "    n_classes=3, class_sep=1.5, random_state=0\n",
        ")\n",
        "\n",
        "ratio_results_synth = {}\n",
        "for r in ratios:\n",
        "    accuracies = []\n",
        "    for seed in range(10):  # 10 repeats\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "            Xs, ys, test_size=r, random_state=seed, stratify=ys\n",
        "        )\n",
        "        pipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=best_k_cv))\n",
        "        pipe.fit(X_tr, y_tr)\n",
        "        y_hat = pipe.predict(X_te)\n",
        "        accuracies.append(accuracy_score(y_te, y_hat))\n",
        "    ratio_results_synth[r] = (np.mean(accuracies), np.std(accuracies))\n",
        "\n",
        "for r in ratios:\n",
        "    mean_acc, std_acc = ratio_results_synth[r]\n",
        "    print(f\"[Synthetic] Test size={int(r*100)}% | Mean Acc over 10 runs = {mean_acc:.4f} ± {std_acc:.4f}\")\n",
        "\n",
        "best_ratio_synth = max(ratio_results_synth, key=lambda rr: ratio_results_synth[rr][0])\n",
        "print(f\"Best test ratio on synthetic data = {int(best_ratio_synth*100)}% \"\n",
        "      f\"(Mean Acc={ratio_results_synth[best_ratio_synth][0]:.4f})\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 6) Predict hand-made samples (from the lab manual)\n",
        "# ----------------------------------------------------\n",
        "print(\"\\n=== Predict hand-made samples ===\")\n",
        "# Train on all Iris data with best k from CV (more data = usually better)\n",
        "final_pipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=best_k_cv))\n",
        "final_pipe.fit(X, y)\n",
        "\n",
        "classes = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
        "x_new = np.array([[1, 1, 1, 1], [4, 3, 1.3, 0.2]])\n",
        "y_new = final_pipe.predict(x_new)\n",
        "for i, pred in enumerate(y_new):\n",
        "    print(f\"x_new[{i}] -> class {pred} -> {classes[pred]}\")\n"
      ]
    }
  ]
}